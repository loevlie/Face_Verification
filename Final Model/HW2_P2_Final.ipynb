{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "id": "h6Gi1R4nnfxG"
   },
   "source": [
    "# HW2 P2 IDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "id": "OZXkCkQAN4GN"
   },
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kagglebnnng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false",
    "id": "dyJfQUjIoirw"
   },
   "outputs": [],
   "source": [
    "! cp kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false",
    "id": "tbOy2oswo3PX"
   },
   "outputs": [],
   "source": [
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B7rUpvASo8wo",
    "outputId": "1a0c4641-a612-41cf-d129-2347cbda157d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 20fall-hw2p2.zip to /home/ubuntu\n",
      "100%|█████████████████████████████████████▉| 1.34G/1.35G [01:16<00:00, 90.7MB/s]\n",
      "100%|██████████████████████████████████████| 1.35G/1.35G [01:16<00:00, 18.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d cmu11785/20fall-hw2p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip 20fall-hw2p2.zip;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "id": "kyznzSkUQHz1"
   },
   "source": [
    "## Imports and Model Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "id": "dBGmKhnNfucr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision   \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSImG-HdG_Y3"
   },
   "source": [
    "# Initializing Weights Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "liEpBXqCG-of"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ2r2zIjwPb6"
   },
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcI5LD_SRPey"
   },
   "source": [
    "## With Metric Learning Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MNcTjZUERTfs"
   },
   "outputs": [],
   "source": [
    "Model_type = 'Baseline'\n",
    "class Normal_Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Normal_Block, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=out_channels,out_channels=out_channels,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.shortcut = nn.Sequential( \n",
    "            nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=3,stride =1, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "                                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.block1(x)\n",
    "        res = self.shortcut(x)\n",
    "        output += res # self.shortcut(x)\n",
    "        return output\n",
    "\n",
    "class Network(nn.Module): # [3,4,6,3] first resNet use block 3 times then the second use it 4 times\n",
    "    def __init__(self, num_feats, embedding_size=256):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_feats,out_channels=64,kernel_size = 7,stride=2,padding=3,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(in_channels=128,out_channels=128,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(in_channels=256,out_channels=256,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels=256,out_channels=256,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels=256,out_channels=256,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "                                    )\n",
    "\n",
    "        # Additional Layers\n",
    "\n",
    "        self.additional_layers = nn.Sequential(Normal_Block(256,512),\n",
    "                                               Normal_Block(512,1024))\n",
    "                                               #Normal_Block(1024,2048))\n",
    "                                               #Normal_Block(2048,2048*2))\n",
    "\n",
    "        self.final_conv = nn.Conv2d(in_channels=1024,out_channels=1024,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.final_BN = nn.BatchNorm2d(1024)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    \n",
    "        self.Linear_layer_embedding = nn.Linear(1024,embedding_size, bias=False)\n",
    "        self.BNf = nn.BatchNorm1d(embedding_size)\n",
    "\n",
    "    def forward(self, x,x2=None,x3=None,eval_mode=False):\n",
    "        \n",
    "        # Baseline layers \n",
    "        output1 = self.layers(x)\n",
    "        output1 = self.additional_layers(output1)\n",
    "        output1 = self.final_BN(self.final_conv(output1))\n",
    "        output1 = output1.reshape(output1.shape[0],output1.shape[1])\n",
    "        embedding_output1 = self.BNf(self.Linear_layer_embedding(output1))\n",
    "\n",
    "\n",
    "\n",
    "         \n",
    "        output2 = self.layers(x2)\n",
    "        output2 = self.additional_layers(output2)\n",
    "        output2 = self.final_BN(self.final_conv(output2))\n",
    "        output2 = output2.reshape(output2.shape[0],output2.shape[1])\n",
    "        embedding_output2 = self.BNf(self.Linear_layer_embedding(output2))\n",
    "        \n",
    "        if eval_mode:\n",
    "            embedding_output2 = None\n",
    "            embedding_output3 = None\n",
    "        else:\n",
    "            output2 = self.layers(x2)\n",
    "            output2 = self.additional_layers(output2)\n",
    "            output2 = self.final_BN(self.final_conv(output2))\n",
    "            output2 = output2.reshape(output2.shape[0],output2.shape[1])\n",
    "            embedding_output2 = self.BNf(self.Linear_layer_embedding(output2))\n",
    "        \n",
    "            output3 = self.layers(x3)\n",
    "            output3 = self.additional_layers(output3)\n",
    "            output3 = self.final_BN(self.final_conv(output3))\n",
    "            output3 = output3.reshape(output3.shape[0],output3.shape[1])\n",
    "            embedding_output3 = self.BNf(self.Linear_layer_embedding(output3))\n",
    "\n",
    "        \n",
    "        return embedding_output1,embedding_output2,embedding_output3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MoW4cPBRUKX"
   },
   "source": [
    "## Normal Approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "C5Vn71p96e-P"
   },
   "outputs": [],
   "source": [
    "Model_type = 'Baseline'\n",
    "class Normal_Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Normal_Block, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=out_channels,out_channels=out_channels,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.shortcut = nn.Sequential( \n",
    "            nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=3,stride =1, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "                                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.block1(x)\n",
    "        res = self.shortcut(x)\n",
    "        output += res # self.shortcut(x)\n",
    "        return output\n",
    "\n",
    "class Network(nn.Module): # [3,4,6,3] first resNet use block 3 times then the second use it 4 times\n",
    "    def __init__(self, num_feats, num_classes, embedding_size=256):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_feats,out_channels=64,kernel_size = 7,stride=2,padding=3,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(in_channels=128,out_channels=128,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(in_channels=256,out_channels=256,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels=256,out_channels=256,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels=256,out_channels=256,kernel_size = 3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "                                    )\n",
    "\n",
    "        # Additional Layers\n",
    "\n",
    "        self.additional_layers = nn.Sequential(Normal_Block(256,512),\n",
    "                                               Normal_Block(512,1024),\n",
    "                                               Normal_Block(1024,2048))\n",
    "                                               #Normal_Block(2048,2048*2))\n",
    "\n",
    "        self.final_conv = nn.Conv2d(in_channels=2048,out_channels=2048,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.final_BN = nn.BatchNorm2d(2048)\n",
    "\n",
    "        self.Linear_layer_embedding = nn.Linear(2048,embedding_size, bias=False)\n",
    "        self.BNf = nn.BatchNorm1d(embedding_size)\n",
    "\n",
    "        # LINEAR LAYER (For Classes)\n",
    "        self.linear_layer_classes = nn.Linear(embedding_size,num_classes,bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x, evalMode=False):\n",
    "        output = self.layers(x)\n",
    "        output = self.additional_layers(output)\n",
    "        output = self.final_BN(self.final_conv(output))\n",
    "        output = output.reshape(output.shape[0],output.shape[1])\n",
    "        embedding_output = self.BNf(self.Linear_layer_embedding(output))\n",
    "\n",
    "        label_output = self.linear_layer_classes(embedding_output)\n",
    "        label_output = label_output/torch.norm(self.linear_layer_classes.weight, dim=1)\n",
    "        \n",
    "        return embedding_output, label_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EspxbW0Q59h"
   },
   "source": [
    "# Creating the Dataset for Training/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bi0euhhMMRMD",
    "outputId": "625da611-ad08-4582-9c85-1907192786ed"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "class ImageDataset_Val(Dataset):\n",
    "    def __init__(self, test_data):\n",
    "        self.img1 = test_data[:,0]\n",
    "        self.img2 = test_data[:,1]\n",
    "        self.test_data = test_data\n",
    "        label_list = test_data[:,2]\n",
    "        labels = np.array([])\n",
    "        for i in label_list:\n",
    "            labels = np.append(labels,np.array(int(i)))\n",
    "        self.label = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img1 = Image.open(self.img1[index])\n",
    "        img1 = torchvision.transforms.ToTensor()(img1)\n",
    "        img2 = Image.open(self.img2[index])\n",
    "        img2 = torchvision.transforms.ToTensor()(img2)\n",
    "        label = torch.Tensor(np.array([self.label[index]]))\n",
    "        return img1, img2, label.squeeze()\n",
    "    \n",
    "import sys\n",
    "from io import StringIO   # StringIO behaves like a file object\n",
    "def parse_test_data(test_file):\n",
    "    img_list = []\n",
    "    \n",
    "    with open(test_file,'r') as g:\n",
    "        test_in = g.read()\n",
    "        \n",
    "    testing = np.genfromtxt(StringIO(test_in),delimiter='\\t',dtype='str',comments=None)\n",
    "    test = [x.split(' ') for x in testing]\n",
    "    return test\n",
    "test_val_data = parse_test_data('verification_pairs_val.txt')\n",
    "test_val_data = np.array(test_val_data)\n",
    "test_val_data.shape\n",
    "test_val_dataset = ImageDataset_Val(test_val_data)\n",
    "test_val_loader = DataLoader(test_val_dataset, batch_size=100, shuffle=False, num_workers=6, drop_last=False)\n",
    "\n",
    "from torchvision import transforms\n",
    "train_dataset = torchvision.datasets.ImageFolder(root='classification_data/train_data', \n",
    "                                                 transform=transforms.ToTensor())\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=256, \n",
    "                                               shuffle=True, num_workers=6,drop_last=True)\n",
    "\n",
    "val_dataset = torchvision.datasets.ImageFolder(root='classification_data/val_data', \n",
    "                                               transform=transforms.ToTensor())\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, \n",
    "                                             shuffle=True, num_workers=6)\n",
    "    \n",
    "    \n",
    "class Dataset_metric(Dataset):\n",
    "    def __init__(self, categories,dataset,root):\n",
    "        self.categories = np.array(dataset.targets)\n",
    "        self.categories_idx = categories\n",
    "        self.dataset = dataset\n",
    "        self.root = root\n",
    "        self.classes = dataset._find_classes(root)\n",
    "    def __len__(self):\n",
    "        return self.dataset.__len__()\n",
    "    def __getitem__(self, idx):\n",
    "        img1,label1 = self.dataset.__getitem__(idx)\n",
    "        Selection_False = np.where(label1 != self.categories)[0]\n",
    "        Classes_Left = self.classes[:label1]+self.classes[label1+1:]\n",
    "        img2_False,img2_False_label = self.dataset.__getitem__(random.choice(Selection_False))\n",
    "        Selection_True = np.where(label1 == self.categories)[0]\n",
    "        img3_true, img3_true_label =  self.dataset.__getitem__(random.choice(Selection_True))\n",
    "        return img1, img3_true,img2_False,label1\n",
    "\n",
    "root_dir = 'classification_data/train_data/'\n",
    "categories = [x for x in range(train_dataset.__len__())]\n",
    "Training_dataset_metric = Dataset_metric(categories,train_dataset,root_dir)\n",
    "neg_stuff = [categories,train_dataset,root_dir]\n",
    "train_dataloader_metric = torch.utils.data.DataLoader(Training_dataset_metric, batch_size=256, \n",
    "                                               shuffle=True, num_workers=6,drop_last=True)\n",
    "\n",
    "root_dir_val = 'classification_data/val_data/'\n",
    "categories_val = [x for x in range(val_dataset.__len__())]\n",
    "Validation_dataset_metric = Dataset_metric(categories_val,val_dataset,root_dir_val)\n",
    "val_dataloader_metric = torch.utils.data.DataLoader(Validation_dataset_metric, batch_size=256, \n",
    "                                             shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PYovI7TGmxX"
   },
   "source": [
    "# Initializing the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OyW9FyrglXFc",
    "outputId": "170a898e-4af8-465a-d6b8-7d2b03fc4d51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 16\n"
     ]
    }
   ],
   "source": [
    "numEpochs = 10\n",
    "num_feats = 3\n",
    "\n",
    "learningRate = 0.15\n",
    "weightDecay = 5e-5\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RsI4eiUMhI8"
   },
   "source": [
    "## For Baseline Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KvGGu699GgVv",
    "outputId": "698664f2-c26c-4fe2-f9ec-351c5aec7993"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (layers): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU()\n",
       "    (14): Dropout(p=0.3, inplace=False)\n",
       "    (15): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (16): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU()\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): ReLU()\n",
       "    (22): Dropout(p=0.3, inplace=False)\n",
       "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU()\n",
       "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU()\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (31): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (32): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (33): ReLU()\n",
       "  )\n",
       "  (additional_layers): Sequential(\n",
       "    (0): Normal_Block(\n",
       "      (block1): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Normal_Block(\n",
       "      (block1): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (final_BN): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (Linear_layer_embedding): Linear(in_features=1024, out_features=512, bias=False)\n",
       "  (BNf): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_net = Network(num_feats=num_feats, embedding_size=512)\n",
    "basic_net.train()\n",
    "basic_net.to(device)\n",
    "basic_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8P8faaiOM8yF"
   },
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Couldn't quite get this one to work.  It trained but took forever.  I tried doing the hard negative and semi-hard negative online mining to help but I didn't get a great score before the final deadline.  It is a cool concept though and I enjoyed learning how to train the model this way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "U4g8vDVDQiqu"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "criterion = nn.TripletMarginLoss(margin=1, p=2)\n",
    "#optimizer_model = torch.optim.SGD(basic_net.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
    "optimizer_model = torch.optim.Adam(basic_net.parameters(),lr=1e-3)# ,weight_decay=weightDecay\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_model, mode='min', factor=0.1, patience=8, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=True)\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "pdist = nn.PairwiseDistance(p=2)\n",
    "def train_metric(model, data_loader, test_loader, task='Classification'):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(numEpochs):\n",
    "        \n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (anchor,positive,negative,label) in enumerate(data_loader):\n",
    "            anchor,positive,negative,label = anchor.to(device),positive.to(device),negative.to(device), label.to(device)\n",
    "            optimizer_model.zero_grad()\n",
    "            model.eval()\n",
    "            embedding_1,embedding_2,embedding_3 = model(anchor,positive,negative)\n",
    "            anchor_hard = []\n",
    "            positive_hard = []\n",
    "            negative_hard = []\n",
    "            #Sim_Neg = cos(embedding_1,embedding_3)\n",
    "            #Sim_Pos = cos(embedding_1,embedding_2)\n",
    "            \n",
    "            Dis_Neg = pdist(embedding_1,embedding_3)\n",
    "            Dis_Pos = pdist(embedding_1,embedding_2)\n",
    "            p = 1\n",
    "            hard = np.where(Dis_Neg.detach().cpu().numpy() < Dis_Pos.detach().cpu().numpy()+p)[0]\n",
    "            anchor = anchor[hard]\n",
    "            positive = positive[hard]\n",
    "            negative = negative[hard]\n",
    "\n",
    "            model.train()\n",
    "            embedding_1,embedding_2,embedding3 = model(anchor,positive,negative)\n",
    "            loss = criterion(embedding_1,embedding_2,embedding3)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_model.step()\n",
    " \n",
    "            avg_loss += loss.item()\n",
    "            if batch_num % 100==99:\n",
    "                scheduler.step(avg_loss/50)\n",
    "                print(f'Scheduler saw loss: {avg_loss/50}')\n",
    "            if batch_num % 50 == 49:\n",
    "                print(f'We got rid of {len(label)-len(hard)} triplets')\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    "                print(f'Time: {(time.time()-start_time)/60} \\n')  \n",
    "                avg_loss = 0\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            del anchor\n",
    "            del negative\n",
    "            del positive\n",
    "            del label\n",
    "            del loss\n",
    "\n",
    "        if task == 'Classification':\n",
    "            print(f'Epoch: {epoch} - Time: {(time.time()-start_time)/60} \\n')\n",
    "            Val_roc = test_verify(model, test_loader)\n",
    "            print(f'ROC Value: {Val_roc}')\n",
    "        else:\n",
    "            test_verify(model, test_loader)\n",
    "        \n",
    "    \n",
    "\n",
    "def test_verify(model, test_loader):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    model.eval()\n",
    "    Cosine_Similarity = []\n",
    "    True_Labels = []\n",
    "    pair_distance = []\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    pdist = nn.PairwiseDistance(p=2)\n",
    "    for batch_num, (img1, img2,labels) in enumerate(test_loader):\n",
    "            img1, img2,labels = img1.to(device), img2.to(device),labels.to(device)\n",
    "            img1_embedding = model(img1,None,None,eval_mode=True)[0]\n",
    "            img2_embedding = model(img2,None,None,eval_mode=True)[0]\n",
    "            torch.cuda.empty_cache()\n",
    "            del img1\n",
    "            del img2\n",
    "            pd = pdist(img1_embedding,img2_embedding)\n",
    "            pair_distance.extend(pd.cpu().detach().numpy())\n",
    "            cosine_similarity = cos(img1_embedding, img2_embedding)\n",
    "            Cosine_Similarity.extend(cosine_similarity.cpu().detach().numpy())\n",
    "            True_Labels.extend(labels.cpu().detach().numpy())\n",
    "            torch.cuda.empty_cache()\n",
    "            del labels\n",
    "            \n",
    "    ROC = roc_auc_score(np.array(True_Labels), np.array(Cosine_Similarity)) # ??????\n",
    "    print(f'ROC with Pairwise Distance {roc_auc_score(np.array(True_Labels),np.array(pair_distance))}')\n",
    "    return ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aXE4BHVUVRXo"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_model = torch.optim.Adam(basic_net.parameters(),lr=1e-3)# ,weight_decay=weightDecay\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_model, mode='min', factor=0.1, patience=8, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=True)\n",
    "def train(model, data_loader, test_loader, task='Classification'):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(numEpochs):\n",
    "        \n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in enumerate(data_loader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            optimizer_model.zero_grad()\n",
    "            outputs,preds = model(feats)\n",
    "            loss = criterion(preds,labels.long())\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_model.step()\n",
    " \n",
    "            avg_loss += loss.item()\n",
    "            if batch_num % 100==99:\n",
    "                scheduler.step(avg_loss/50)\n",
    "                print(f'Scheduler saw loss: {avg_loss/50}')\n",
    "            if batch_num % 50 == 49:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    "                print(f'Time: {(time.time()-start_time)/60} \\n')  \n",
    "                avg_loss = 0\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "\n",
    "        if task == 'Classification':\n",
    "            print(f'Epoch: {epoch} - Time: {(time.time()-start_time)/60} \\n')\n",
    "            Val_roc = test_verify(model, test_val_loader)\n",
    "            print(f'ROC Value: {Val_roc}')\n",
    "            testing_loss_class,testing_acc_val = test_classify(model,val_dataloader)\n",
    "            \n",
    "            print(f'Validation loss: {testing_loss_class}, validation accuracy: {testing_acc_val}')\n",
    "        else:\n",
    "            test_verify(model, test_loader)\n",
    "        \n",
    "    \n",
    "\n",
    "def test_verify(model, test_loader):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    model.eval()\n",
    "    Cosine_Similarity = []\n",
    "    True_Labels = []\n",
    "    pair_distance = []\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    pdist = nn.PairwiseDistance(p=2)\n",
    "    for batch_num, (img1, img2,labels) in enumerate(test_loader):\n",
    "            img1, img2,labels = img1.to(device), img2.to(device),labels.to(device)\n",
    "            img1_embedding = model(img1)[0]\n",
    "            torch.cuda.empty_cache()\n",
    "            del img1\n",
    "            img2_embedding = model(img2)[0]\n",
    "            pd = pdist(img1_embedding,img2_embedding)\n",
    "            pair_distance.extend(pd.cpu().detach().numpy())\n",
    "            cosine_similarity = cos(img1_embedding, img2_embedding)\n",
    "            Cosine_Similarity.extend(cosine_similarity.cpu().detach().numpy())\n",
    "            True_Labels.extend(labels.cpu().detach().numpy())\n",
    "            torch.cuda.empty_cache()\n",
    "            del img2\n",
    "            del labels\n",
    "    ROC = roc_auc_score(np.array(True_Labels), np.array(Cosine_Similarity)) # ??????\n",
    "    print(f'ROC with Pairwise Distance {roc_auc_score(np.array(True_Labels),np.array(pair_distance))}')\n",
    "    return ROC\n",
    "\n",
    "def test_classify(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "        feats, labels = feats.to(device), labels.to(device)\n",
    "        outputs = model(feats)[1]\n",
    "        \n",
    "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, labels.long())\n",
    "        \n",
    "        accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "        test_loss.extend([loss.item()]*feats.size()[0])\n",
    "        del feats\n",
    "        del labels\n",
    "\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracy/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpqPY9n5NXgW"
   },
   "source": [
    "## Submission Function to generate the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kN0F6mzkPY2o"
   },
   "outputs": [],
   "source": [
    "def Submission(model,num):\n",
    "    import sys\n",
    "    from io import StringIO   # StringIO behaves like a file object\n",
    "    def parse_test_data(test_file):\n",
    "        img_list = []\n",
    "\n",
    "        with open(test_file,'r') as g:\n",
    "            test_in = g.read()\n",
    "\n",
    "        testing = np.genfromtxt(StringIO(test_in),delimiter='\\t',dtype='str',comments=None)\n",
    "        test = [x.split(' ') for x in testing]\n",
    "        return test\n",
    "    test_data = parse_test_data('verification_pairs_test.txt')\n",
    "    test_data = np.array(test_data)\n",
    "\n",
    "    class ImageDataset(Dataset):\n",
    "        def __init__(self, test_data):\n",
    "            self.img1 = test_data[:,0]\n",
    "            self.img2 = test_data[:,1]\n",
    "            self.test_data = test_data\n",
    "        def __len__(self):\n",
    "            return len(self.img1)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            img1 = Image.open(self.img1[index])\n",
    "            img1 = torchvision.transforms.ToTensor()(img1)\n",
    "            img2 = Image.open(self.img2[index])\n",
    "            img2 = torchvision.transforms.ToTensor()(img2)\n",
    "            return img1, img2\n",
    "\n",
    "\n",
    "    test_dataset = ImageDataset(test_data)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=6, drop_last=False)\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    model.eval()\n",
    "    Cosine_Similarity = []\n",
    "    True_Labels = []\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    path_pairs = []\n",
    "    for batch_num, (img1, img2) in enumerate(test_loader):\n",
    "            img1, img2 = img1.to(device), img2.to(device)\n",
    "            img1_embedding = model(img1,None,None,eval_mode=True)[0]\n",
    "            img2_embedding = model(img2,None,None,eval_mode=True)[0]\n",
    "            cosine_similarity = cos(img1_embedding, img2_embedding)\n",
    "            Cosine_Similarity.append(cosine_similarity.cpu().detach().numpy())\n",
    "            torch.cuda.empty_cache()\n",
    "            del img1\n",
    "            del img2\n",
    "    similaritys_test = np.concatenate(np.array(Cosine_Similarity))\n",
    "    paths = [' '.join(list(test_data[x])) for x in range(len(test_data))]\n",
    "    ID = np.array(paths)\n",
    "    df_pred = pd.DataFrame(data={'Id':ID,'Category':similaritys_test})\n",
    "    df_pred.to_csv(f'Prediction.csv',index=False)\n",
    "    !kaggle competitions submit -c 11785-hw2p2-slack-kaggle -f Prediction.csv -m f\"Submission {num}\"\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s31tIf0PEm_"
   },
   "source": [
    "## Running the Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nL_My4yQTw5",
    "outputId": "54a8db58-0052-4b65-a995-efc5eff56ed4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got rid of 130 triplets\n",
      "Epoch: 1\tBatch: 50\tAvg-Loss: 1.0245\n",
      "Time: 0.3696061174074809 \n",
      "\n",
      "Scheduler saw loss: 0.9727894949913025\n",
      "We got rid of 116 triplets\n",
      "Epoch: 1\tBatch: 100\tAvg-Loss: 0.9728\n",
      "Time: 0.7192223151524861 \n",
      "\n",
      "We got rid of 94 triplets\n",
      "Epoch: 1\tBatch: 150\tAvg-Loss: 0.9665\n",
      "Time: 1.074746572971344 \n",
      "\n",
      "Scheduler saw loss: 0.9516123521327973\n",
      "We got rid of 101 triplets\n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.9516\n",
      "Time: 1.4317198912302653 \n",
      "\n",
      "We got rid of 113 triplets\n",
      "Epoch: 1\tBatch: 250\tAvg-Loss: 0.9462\n",
      "Time: 1.7957467794418336 \n",
      "\n",
      "Scheduler saw loss: 0.935616010427475\n",
      "We got rid of 113 triplets\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 0.9356\n",
      "Time: 2.162666996320089 \n",
      "\n",
      "We got rid of 104 triplets\n",
      "Epoch: 1\tBatch: 350\tAvg-Loss: 0.9323\n",
      "Time: 2.5195201873779296 \n",
      "\n",
      "Scheduler saw loss: 0.9386372590065002\n",
      "We got rid of 115 triplets\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.9386\n",
      "Time: 2.8709099610646565 \n",
      "\n",
      "We got rid of 122 triplets\n",
      "Epoch: 1\tBatch: 450\tAvg-Loss: 0.9495\n",
      "Time: 3.219114085038503 \n",
      "\n",
      "Scheduler saw loss: 0.9308745837211609\n",
      "We got rid of 110 triplets\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 0.9309\n",
      "Time: 3.569627888997396 \n",
      "\n",
      "We got rid of 114 triplets\n",
      "Epoch: 1\tBatch: 550\tAvg-Loss: 0.9499\n",
      "Time: 3.920508603254954 \n",
      "\n",
      "Scheduler saw loss: 0.93478649020195\n",
      "We got rid of 129 triplets\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.9348\n",
      "Time: 4.2675418297449745 \n",
      "\n",
      "We got rid of 119 triplets\n",
      "Epoch: 1\tBatch: 650\tAvg-Loss: 0.9278\n",
      "Time: 4.615960824489593 \n",
      "\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Scheduler saw loss: 0.9363801097869873\n",
      "We got rid of 116 triplets\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 0.9364\n",
      "Time: 4.966845862070719 \n",
      "\n",
      "We got rid of 124 triplets\n",
      "Epoch: 1\tBatch: 750\tAvg-Loss: 0.9177\n",
      "Time: 5.319178104400635 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "train_metric(basic_net,train_dataloader_metric,test_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "srv6EUbifumx",
    "outputId": "5cd1a352-42fb-4287-913f-32e60fb9b992"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC with Pairwise Distance 0.2802486018885795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7032861489546747"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_verify(basic_net,test_val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Jr1E6-iCHIGy",
    "outputId": "9d70692e-19df-4e8c-cc61-e1442d4b6277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 50\tAvg-Loss: 8.8420\n",
      "Time: 0.23900279998779297 \n",
      "\n",
      "Scheduler saw loss: 8.946311588287353\n",
      "Epoch: 1\tBatch: 100\tAvg-Loss: 8.9463\n",
      "Time: 0.46799528201421103 \n",
      "\n",
      "Epoch: 1\tBatch: 150\tAvg-Loss: 8.9689\n",
      "Time: 0.6967129508654276 \n",
      "\n",
      "Scheduler saw loss: 9.001800155639648\n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 9.0018\n",
      "Time: 0.9256191809972127 \n",
      "\n",
      "Epoch: 1\tBatch: 250\tAvg-Loss: 9.0695\n",
      "Time: 1.1542069832483928 \n",
      "\n",
      "Scheduler saw loss: 9.101495552062989\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 9.1015\n",
      "Time: 1.3824779907862346 \n",
      "\n",
      "Epoch: 1\tBatch: 350\tAvg-Loss: 9.0594\n",
      "Time: 1.6108234763145446 \n",
      "\n",
      "Scheduler saw loss: 9.013768711090087\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 9.0138\n",
      "Time: 1.8391965508461 \n",
      "\n",
      "Epoch: 1\tBatch: 450\tAvg-Loss: 9.0917\n",
      "Time: 2.0677988290786744 \n",
      "\n",
      "Scheduler saw loss: 8.961400547027587\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 8.9614\n",
      "Time: 2.296770966053009 \n",
      "\n",
      "Epoch: 1\tBatch: 550\tAvg-Loss: 8.8841\n",
      "Time: 2.525223195552826 \n",
      "\n",
      "Scheduler saw loss: 8.991526012420655\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 8.9915\n",
      "Time: 2.753726116816203 \n",
      "\n",
      "Epoch: 1\tBatch: 650\tAvg-Loss: 9.0355\n",
      "Time: 2.9821442008018493 \n",
      "\n",
      "Scheduler saw loss: 9.006270503997802\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 9.0063\n",
      "Time: 3.2105616450309755 \n",
      "\n",
      "Epoch: 1\tBatch: 750\tAvg-Loss: 8.8762\n",
      "Time: 3.4390965342521667 \n",
      "\n",
      "Scheduler saw loss: 8.874852409362793\n",
      "Epoch: 1\tBatch: 800\tAvg-Loss: 8.8749\n",
      "Time: 3.6674051880836487 \n",
      "\n",
      "Epoch: 1\tBatch: 850\tAvg-Loss: 8.8628\n",
      "Time: 3.8955092589060465 \n",
      "\n",
      "Scheduler saw loss: 8.776064929962159\n",
      "Epoch: 1\tBatch: 900\tAvg-Loss: 8.7761\n",
      "Time: 4.124313771724701 \n",
      "\n",
      "Epoch: 1\tBatch: 950\tAvg-Loss: 8.8662\n",
      "Time: 4.35294524828593 \n",
      "\n",
      "Scheduler saw loss: 8.723237152099609\n",
      "Epoch: 1\tBatch: 1000\tAvg-Loss: 8.7232\n",
      "Time: 4.5813498814900715 \n",
      "\n",
      "Epoch: 1\tBatch: 1050\tAvg-Loss: 8.6788\n",
      "Time: 4.8098718841870625 \n",
      "\n",
      "Scheduler saw loss: 8.657250843048097\n",
      "Epoch: 1\tBatch: 1100\tAvg-Loss: 8.6573\n",
      "Time: 5.038802842299144 \n",
      "\n",
      "Epoch: 1\tBatch: 1150\tAvg-Loss: 8.5274\n",
      "Time: 5.26743795077006 \n",
      "\n",
      "Scheduler saw loss: 8.532039699554444\n",
      "Epoch: 1\tBatch: 1200\tAvg-Loss: 8.5320\n",
      "Time: 5.495621999104817 \n",
      "\n",
      "Epoch: 1\tBatch: 1250\tAvg-Loss: 8.5471\n",
      "Time: 5.724026934305827 \n",
      "\n",
      "Scheduler saw loss: 8.479204273223877\n",
      "Epoch: 1\tBatch: 1300\tAvg-Loss: 8.4792\n",
      "Time: 5.952349801858266 \n",
      "\n",
      "Epoch: 1\tBatch: 1350\tAvg-Loss: 8.4691\n",
      "Time: 6.180681280295054 \n",
      "\n",
      "Scheduler saw loss: 8.377327842712402\n",
      "Epoch: 1\tBatch: 1400\tAvg-Loss: 8.3773\n",
      "Time: 6.409094047546387 \n",
      "\n",
      "Epoch: 1\tBatch: 1450\tAvg-Loss: 8.3592\n",
      "Time: 6.637299811840057 \n",
      "\n",
      "Epoch: 0 - Time: 6.804027517636617 \n",
      "\n",
      "ROC with Pairwise Distance 0.2982883554803129\n",
      "ROC Value: 0.6456817131929588\n",
      "Validation loss: 107.17963879394532, validation accuracy: 0.00075\n",
      "Epoch: 2\tBatch: 50\tAvg-Loss: 8.3155\n",
      "Time: 7.3524915933609005 \n",
      "\n",
      "Scheduler saw loss: 8.24933780670166\n",
      "Epoch: 2\tBatch: 100\tAvg-Loss: 8.2493\n",
      "Time: 7.580071731408437 \n",
      "\n",
      "Epoch: 2\tBatch: 150\tAvg-Loss: 8.1877\n",
      "Time: 7.80761893192927 \n",
      "\n",
      "Scheduler saw loss: 8.091631488800049\n",
      "Epoch: 2\tBatch: 200\tAvg-Loss: 8.0916\n",
      "Time: 8.0351318359375 \n",
      "\n",
      "Epoch: 2\tBatch: 250\tAvg-Loss: 8.0744\n",
      "Time: 8.262804607550303 \n",
      "\n",
      "Scheduler saw loss: 8.19005760192871\n",
      "Epoch: 2\tBatch: 300\tAvg-Loss: 8.1901\n",
      "Time: 8.490426309903462 \n",
      "\n",
      "Epoch: 2\tBatch: 350\tAvg-Loss: 8.0770\n",
      "Time: 8.718188198407491 \n",
      "\n",
      "Scheduler saw loss: 8.020344934463502\n",
      "Epoch: 2\tBatch: 400\tAvg-Loss: 8.0203\n",
      "Time: 8.945903825759888 \n",
      "\n",
      "Epoch: 2\tBatch: 450\tAvg-Loss: 7.9842\n",
      "Time: 9.173872562249501 \n",
      "\n",
      "Scheduler saw loss: 7.939312839508057\n",
      "Epoch: 2\tBatch: 500\tAvg-Loss: 7.9393\n",
      "Time: 9.401933828989664 \n",
      "\n",
      "Epoch: 2\tBatch: 550\tAvg-Loss: 7.9097\n",
      "Time: 9.629849135875702 \n",
      "\n",
      "Scheduler saw loss: 7.811976594924927\n",
      "Epoch: 2\tBatch: 600\tAvg-Loss: 7.8120\n",
      "Time: 9.857494664192199 \n",
      "\n",
      "Epoch: 2\tBatch: 650\tAvg-Loss: 7.7386\n",
      "Time: 10.085202284653981 \n",
      "\n",
      "Scheduler saw loss: 7.671202154159546\n",
      "Epoch: 2\tBatch: 700\tAvg-Loss: 7.6712\n",
      "Time: 10.312894904613495 \n",
      "\n",
      "Epoch: 2\tBatch: 750\tAvg-Loss: 7.6104\n",
      "Time: 10.540467965602875 \n",
      "\n",
      "Scheduler saw loss: 7.548516321182251\n",
      "Epoch: 2\tBatch: 800\tAvg-Loss: 7.5485\n",
      "Time: 10.76794360478719 \n",
      "\n",
      "Epoch: 2\tBatch: 850\tAvg-Loss: 7.5248\n",
      "Time: 10.995398076375325 \n",
      "\n",
      "Scheduler saw loss: 7.475100889205932\n",
      "Epoch: 2\tBatch: 900\tAvg-Loss: 7.4751\n",
      "Time: 11.222976990540822 \n",
      "\n",
      "Epoch: 2\tBatch: 950\tAvg-Loss: 7.4549\n",
      "Time: 11.450422517458598 \n",
      "\n",
      "Scheduler saw loss: 7.415381526947021\n",
      "Epoch: 2\tBatch: 1000\tAvg-Loss: 7.4154\n",
      "Time: 11.678333926200867 \n",
      "\n",
      "Epoch: 2\tBatch: 1050\tAvg-Loss: 7.3637\n",
      "Time: 11.9058989127477 \n",
      "\n",
      "Scheduler saw loss: 7.351924934387207\n",
      "Epoch: 2\tBatch: 1100\tAvg-Loss: 7.3519\n",
      "Time: 12.133385868867238 \n",
      "\n",
      "Epoch: 2\tBatch: 1150\tAvg-Loss: 7.3094\n",
      "Time: 12.360970989863079 \n",
      "\n",
      "Scheduler saw loss: 7.302326679229736\n",
      "Epoch: 2\tBatch: 1200\tAvg-Loss: 7.3023\n",
      "Time: 12.588593924045563 \n",
      "\n",
      "Epoch: 2\tBatch: 1250\tAvg-Loss: 7.2360\n",
      "Time: 12.816214863459269 \n",
      "\n",
      "Scheduler saw loss: 7.212185459136963\n",
      "Epoch: 2\tBatch: 1300\tAvg-Loss: 7.2122\n",
      "Time: 13.043780422210693 \n",
      "\n",
      "Epoch: 2\tBatch: 1350\tAvg-Loss: 7.1879\n",
      "Time: 13.271411271890004 \n",
      "\n",
      "Scheduler saw loss: 7.142216882705688\n",
      "Epoch: 2\tBatch: 1400\tAvg-Loss: 7.1422\n",
      "Time: 13.500077390670777 \n",
      "\n",
      "Epoch: 2\tBatch: 1450\tAvg-Loss: 7.1190\n",
      "Time: 13.727998872598013 \n",
      "\n",
      "Epoch: 1 - Time: 13.894381268819172 \n",
      "\n",
      "ROC with Pairwise Distance 0.235733818166803\n",
      "ROC Value: 0.7592620139514359\n",
      "Validation loss: 7.2060239791870115, validation accuracy: 0.005\n",
      "Epoch: 3\tBatch: 50\tAvg-Loss: 7.0055\n",
      "Time: 14.435653130213419 \n",
      "\n",
      "Scheduler saw loss: 7.0287837791442875\n",
      "Epoch: 3\tBatch: 100\tAvg-Loss: 7.0288\n",
      "Time: 14.664040406545004 \n",
      "\n",
      "Epoch: 3\tBatch: 150\tAvg-Loss: 6.9867\n",
      "Time: 14.89248864253362 \n",
      "\n",
      "Scheduler saw loss: 6.964216852188111\n",
      "Epoch: 3\tBatch: 200\tAvg-Loss: 6.9642\n",
      "Time: 15.121013160546621 \n",
      "\n",
      "Epoch: 3\tBatch: 250\tAvg-Loss: 6.9483\n",
      "Time: 15.34966193040212 \n",
      "\n",
      "Scheduler saw loss: 6.93212555885315\n",
      "Epoch: 3\tBatch: 300\tAvg-Loss: 6.9321\n",
      "Time: 15.578031369050343 \n",
      "\n",
      "Epoch: 3\tBatch: 350\tAvg-Loss: 6.8961\n",
      "Time: 15.806589261690776 \n",
      "\n",
      "Scheduler saw loss: 6.854155654907227\n",
      "Epoch: 3\tBatch: 400\tAvg-Loss: 6.8542\n",
      "Time: 16.035397458076478 \n",
      "\n",
      "Epoch: 3\tBatch: 450\tAvg-Loss: 6.8522\n",
      "Time: 16.264398765563964 \n",
      "\n",
      "Scheduler saw loss: 6.837081470489502\n",
      "Epoch: 3\tBatch: 500\tAvg-Loss: 6.8371\n",
      "Time: 16.492884890238443 \n",
      "\n",
      "Epoch: 3\tBatch: 550\tAvg-Loss: 6.8026\n",
      "Time: 16.721972600618997 \n",
      "\n",
      "Scheduler saw loss: 6.774249849319458\n",
      "Epoch: 3\tBatch: 600\tAvg-Loss: 6.7742\n",
      "Time: 16.95020268758138 \n",
      "\n",
      "Epoch: 3\tBatch: 650\tAvg-Loss: 6.7562\n",
      "Time: 17.178546086947122 \n",
      "\n",
      "Scheduler saw loss: 6.7138924980163575\n",
      "Epoch: 3\tBatch: 700\tAvg-Loss: 6.7139\n",
      "Time: 17.407097971439363 \n",
      "\n",
      "Epoch: 3\tBatch: 750\tAvg-Loss: 6.7059\n",
      "Time: 17.63553045988083 \n",
      "\n",
      "Scheduler saw loss: 6.699488134384155\n",
      "Epoch: 3\tBatch: 800\tAvg-Loss: 6.6995\n",
      "Time: 17.864346313476563 \n",
      "\n",
      "Epoch: 3\tBatch: 850\tAvg-Loss: 6.6658\n",
      "Time: 18.092755615711212 \n",
      "\n",
      "Scheduler saw loss: 6.635871124267578\n",
      "Epoch: 3\tBatch: 900\tAvg-Loss: 6.6359\n",
      "Time: 18.321028172969818 \n",
      "\n",
      "Epoch: 3\tBatch: 950\tAvg-Loss: 6.5896\n",
      "Time: 18.549234739939372 \n",
      "\n",
      "Scheduler saw loss: 6.565855169296265\n",
      "Epoch: 3\tBatch: 1000\tAvg-Loss: 6.5659\n",
      "Time: 18.77750792900721 \n",
      "\n",
      "Epoch: 3\tBatch: 1050\tAvg-Loss: 6.5215\n",
      "Time: 19.00571036338806 \n",
      "\n",
      "Scheduler saw loss: 6.4829208946228025\n",
      "Epoch: 3\tBatch: 1100\tAvg-Loss: 6.4829\n",
      "Time: 19.233963720003764 \n",
      "\n",
      "Epoch: 3\tBatch: 1150\tAvg-Loss: 6.4327\n",
      "Time: 19.462326395511628 \n",
      "\n",
      "Scheduler saw loss: 6.4269711208343505\n",
      "Epoch: 3\tBatch: 1200\tAvg-Loss: 6.4270\n",
      "Time: 19.690568685531616 \n",
      "\n",
      "Epoch: 3\tBatch: 1250\tAvg-Loss: 6.3721\n",
      "Time: 19.918914647897086 \n",
      "\n",
      "Scheduler saw loss: 6.340483798980713\n",
      "Epoch: 3\tBatch: 1300\tAvg-Loss: 6.3405\n",
      "Time: 20.14719698826472 \n",
      "\n",
      "Epoch: 3\tBatch: 1350\tAvg-Loss: 6.3273\n",
      "Time: 20.37541244029999 \n",
      "\n",
      "Scheduler saw loss: 6.298536109924316\n",
      "Epoch: 3\tBatch: 1400\tAvg-Loss: 6.2985\n",
      "Time: 20.603902649879455 \n",
      "\n",
      "Epoch: 3\tBatch: 1450\tAvg-Loss: 6.2650\n",
      "Time: 20.832510018348692 \n",
      "\n",
      "Epoch: 2 - Time: 20.999345107873282 \n",
      "\n",
      "ROC with Pairwise Distance 0.18318880375341623\n",
      "ROC Value: 0.8099919481169691\n",
      "Validation loss: 6.45284426498413, validation accuracy: 0.018\n",
      "Epoch: 4\tBatch: 50\tAvg-Loss: 6.1462\n",
      "Time: 21.54218407869339 \n",
      "\n",
      "Scheduler saw loss: 6.122394208908081\n",
      "Epoch: 4\tBatch: 100\tAvg-Loss: 6.1224\n",
      "Time: 21.770501716931662 \n",
      "\n",
      "Epoch: 4\tBatch: 150\tAvg-Loss: 6.1440\n",
      "Time: 21.99940243164698 \n",
      "\n",
      "Scheduler saw loss: 6.103599014282227\n",
      "Epoch: 4\tBatch: 200\tAvg-Loss: 6.1036\n",
      "Time: 22.228045129776 \n",
      "\n",
      "Epoch: 4\tBatch: 250\tAvg-Loss: 6.1069\n",
      "Time: 22.45666233698527 \n",
      "\n",
      "Scheduler saw loss: 6.0882204818725585\n",
      "Epoch: 4\tBatch: 300\tAvg-Loss: 6.0882\n",
      "Time: 22.684857706228893 \n",
      "\n",
      "Epoch: 4\tBatch: 350\tAvg-Loss: 6.0746\n",
      "Time: 22.913241231441496 \n",
      "\n",
      "Scheduler saw loss: 6.028361730575561\n",
      "Epoch: 4\tBatch: 400\tAvg-Loss: 6.0284\n",
      "Time: 23.141602957248686 \n",
      "\n",
      "Epoch: 4\tBatch: 450\tAvg-Loss: 6.0494\n",
      "Time: 23.37003100713094 \n",
      "\n",
      "Scheduler saw loss: 6.0139087963104245\n",
      "Epoch: 4\tBatch: 500\tAvg-Loss: 6.0139\n",
      "Time: 23.59853664636612 \n",
      "\n",
      "Epoch: 4\tBatch: 550\tAvg-Loss: 5.9923\n",
      "Time: 23.82683676481247 \n",
      "\n",
      "Scheduler saw loss: 5.973349151611328\n",
      "Epoch: 4\tBatch: 600\tAvg-Loss: 5.9733\n",
      "Time: 24.05503072341283 \n",
      "\n",
      "Epoch: 4\tBatch: 650\tAvg-Loss: 5.9550\n",
      "Time: 24.283810520172118 \n",
      "\n",
      "Scheduler saw loss: 5.910103034973145\n",
      "Epoch: 4\tBatch: 700\tAvg-Loss: 5.9101\n",
      "Time: 24.512251019477844 \n",
      "\n",
      "Epoch: 4\tBatch: 750\tAvg-Loss: 5.8865\n",
      "Time: 24.741520567735037 \n",
      "\n",
      "Scheduler saw loss: 5.894420948028564\n",
      "Epoch: 4\tBatch: 800\tAvg-Loss: 5.8944\n",
      "Time: 24.97042200167974 \n",
      "\n",
      "Epoch: 4\tBatch: 850\tAvg-Loss: 5.8675\n",
      "Time: 25.19867570400238 \n",
      "\n",
      "Scheduler saw loss: 5.869185571670532\n",
      "Epoch: 4\tBatch: 900\tAvg-Loss: 5.8692\n",
      "Time: 25.42705105940501 \n",
      "\n",
      "Epoch: 4\tBatch: 950\tAvg-Loss: 5.7980\n",
      "Time: 25.655460568269095 \n",
      "\n",
      "Scheduler saw loss: 5.816000432968139\n",
      "Epoch: 4\tBatch: 1000\tAvg-Loss: 5.8160\n",
      "Time: 25.884070654710133 \n",
      "\n",
      "Epoch: 4\tBatch: 1050\tAvg-Loss: 5.8056\n",
      "Time: 26.11240617831548 \n",
      "\n",
      "Scheduler saw loss: 5.763643779754639\n",
      "Epoch: 4\tBatch: 1100\tAvg-Loss: 5.7636\n",
      "Time: 26.34098043044408 \n",
      "\n",
      "Epoch: 4\tBatch: 1150\tAvg-Loss: 5.7486\n",
      "Time: 26.569468649228416 \n",
      "\n",
      "Scheduler saw loss: 5.743552322387695\n",
      "Epoch: 4\tBatch: 1200\tAvg-Loss: 5.7436\n",
      "Time: 26.79776712656021 \n",
      "\n",
      "Epoch: 4\tBatch: 1250\tAvg-Loss: 5.7154\n",
      "Time: 27.026136008898415 \n",
      "\n",
      "Scheduler saw loss: 5.692786598205567\n",
      "Epoch: 4\tBatch: 1300\tAvg-Loss: 5.6928\n",
      "Time: 27.254681901137033 \n",
      "\n",
      "Epoch: 4\tBatch: 1350\tAvg-Loss: 5.7020\n",
      "Time: 27.48313568433126 \n",
      "\n",
      "Scheduler saw loss: 5.6414612197875975\n",
      "Epoch: 4\tBatch: 1400\tAvg-Loss: 5.6415\n",
      "Time: 27.711683130264284 \n",
      "\n",
      "Epoch: 4\tBatch: 1450\tAvg-Loss: 5.6204\n",
      "Time: 27.940403429667153 \n",
      "\n",
      "Epoch: 3 - Time: 28.10736624399821 \n",
      "\n",
      "ROC with Pairwise Distance 0.14199815735753718\n",
      "ROC Value: 0.8223920286357994\n",
      "Validation loss: 5.730693664550781, validation accuracy: 0.044625\n",
      "Epoch: 5\tBatch: 50\tAvg-Loss: 5.5207\n",
      "Time: 28.648431885242463 \n",
      "\n",
      "Scheduler saw loss: 5.525049867630005\n",
      "Epoch: 5\tBatch: 100\tAvg-Loss: 5.5250\n",
      "Time: 28.876643776893616 \n",
      "\n",
      "Epoch: 5\tBatch: 150\tAvg-Loss: 5.4959\n",
      "Time: 29.104583863417307 \n",
      "\n",
      "Scheduler saw loss: 5.53263445854187\n",
      "Epoch: 5\tBatch: 200\tAvg-Loss: 5.5326\n",
      "Time: 29.332553295294442 \n",
      "\n",
      "Epoch: 5\tBatch: 250\tAvg-Loss: 5.5475\n",
      "Time: 29.560514533519743 \n",
      "\n",
      "Scheduler saw loss: 5.533153228759765\n",
      "Epoch: 5\tBatch: 300\tAvg-Loss: 5.5332\n",
      "Time: 29.788444956143696 \n",
      "\n",
      "Epoch: 5\tBatch: 350\tAvg-Loss: 5.4788\n",
      "Time: 30.01646787325541 \n",
      "\n",
      "Scheduler saw loss: 5.516521949768066\n",
      "Epoch: 5\tBatch: 400\tAvg-Loss: 5.5165\n",
      "Time: 30.24449520111084 \n",
      "\n",
      "Epoch: 5\tBatch: 450\tAvg-Loss: 5.4650\n",
      "Time: 30.472731641928355 \n",
      "\n",
      "Scheduler saw loss: 5.459552946090699\n",
      "Epoch: 5\tBatch: 500\tAvg-Loss: 5.4596\n",
      "Time: 30.701081093152364 \n",
      "\n",
      "Epoch: 5\tBatch: 550\tAvg-Loss: 5.4406\n",
      "Time: 30.929145407676696 \n",
      "\n",
      "Scheduler saw loss: 5.478934125900269\n",
      "Epoch: 5\tBatch: 600\tAvg-Loss: 5.4789\n",
      "Time: 31.15714108546575 \n",
      "\n",
      "Epoch: 5\tBatch: 650\tAvg-Loss: 5.4380\n",
      "Time: 31.38509957790375 \n",
      "\n",
      "Scheduler saw loss: 5.384798755645752\n",
      "Epoch: 5\tBatch: 700\tAvg-Loss: 5.3848\n",
      "Time: 31.61304922501246 \n",
      "\n",
      "Epoch: 5\tBatch: 750\tAvg-Loss: 5.4364\n",
      "Time: 31.841070612271626 \n",
      "\n",
      "Scheduler saw loss: 5.429769659042359\n",
      "Epoch: 5\tBatch: 800\tAvg-Loss: 5.4298\n",
      "Time: 32.069173383712766 \n",
      "\n",
      "Epoch: 5\tBatch: 850\tAvg-Loss: 5.4076\n",
      "Time: 32.297198474407196 \n",
      "\n",
      "Scheduler saw loss: 5.374808158874512\n",
      "Epoch: 5\tBatch: 900\tAvg-Loss: 5.3748\n",
      "Time: 32.52517505089442 \n",
      "\n",
      "Epoch: 5\tBatch: 950\tAvg-Loss: 5.3430\n",
      "Time: 32.75459166367849 \n",
      "\n",
      "Scheduler saw loss: 5.374165706634521\n",
      "Epoch: 5\tBatch: 1000\tAvg-Loss: 5.3742\n",
      "Time: 32.983066737651825 \n",
      "\n",
      "Epoch: 5\tBatch: 1050\tAvg-Loss: 5.3615\n",
      "Time: 33.21132651567459 \n",
      "\n",
      "Scheduler saw loss: 5.322793035507202\n",
      "Epoch: 5\tBatch: 1100\tAvg-Loss: 5.3228\n",
      "Time: 33.43940576314926 \n",
      "\n",
      "Epoch: 5\tBatch: 1150\tAvg-Loss: 5.3292\n",
      "Time: 33.667652396361035 \n",
      "\n",
      "Scheduler saw loss: 5.307854318618775\n",
      "Epoch: 5\tBatch: 1200\tAvg-Loss: 5.3079\n",
      "Time: 33.89582672516505 \n",
      "\n",
      "Epoch: 5\tBatch: 1250\tAvg-Loss: 5.2839\n",
      "Time: 34.12442495822906 \n",
      "\n",
      "Scheduler saw loss: 5.294965362548828\n",
      "Epoch: 5\tBatch: 1300\tAvg-Loss: 5.2950\n",
      "Time: 34.35243694782257 \n",
      "\n",
      "Epoch: 5\tBatch: 1350\tAvg-Loss: 5.3129\n",
      "Time: 34.5804625749588 \n",
      "\n",
      "Scheduler saw loss: 5.287248764038086\n",
      "Epoch: 5\tBatch: 1400\tAvg-Loss: 5.2872\n",
      "Time: 34.80843620300293 \n",
      "\n",
      "Epoch: 5\tBatch: 1450\tAvg-Loss: 5.2725\n",
      "Time: 35.03653059800466 \n",
      "\n",
      "Epoch: 4 - Time: 35.20321867863337 \n",
      "\n",
      "ROC with Pairwise Distance 0.12815802852740865\n",
      "ROC Value: 0.8390848209101209\n",
      "Validation loss: 5.396927871704102, validation accuracy: 0.065875\n",
      "Epoch: 6\tBatch: 50\tAvg-Loss: 5.1191\n",
      "Time: 35.74411604007085 \n",
      "\n",
      "Scheduler saw loss: 5.097092618942261\n",
      "Epoch: 6\tBatch: 100\tAvg-Loss: 5.0971\n",
      "Time: 35.97243323723475 \n",
      "\n",
      "Epoch: 6\tBatch: 150\tAvg-Loss: 5.1401\n",
      "Time: 36.20090395609538 \n",
      "\n",
      "Scheduler saw loss: 5.136799507141113\n",
      "Epoch: 6\tBatch: 200\tAvg-Loss: 5.1368\n",
      "Time: 36.429368964831035 \n",
      "\n",
      "Epoch: 6\tBatch: 250\tAvg-Loss: 5.1520\n",
      "Time: 36.6582877834638 \n",
      "\n",
      "Scheduler saw loss: 5.144047708511352\n",
      "Epoch: 6\tBatch: 300\tAvg-Loss: 5.1440\n",
      "Time: 36.88695307572683 \n",
      "\n",
      "Epoch: 6\tBatch: 350\tAvg-Loss: 5.1459\n",
      "Time: 37.11552469333013 \n",
      "\n",
      "Scheduler saw loss: 5.156480655670166\n",
      "Epoch: 6\tBatch: 400\tAvg-Loss: 5.1565\n",
      "Time: 37.34385091463725 \n",
      "\n",
      "Epoch: 6\tBatch: 450\tAvg-Loss: 5.1706\n",
      "Time: 37.57229437033335 \n",
      "\n",
      "Scheduler saw loss: 5.149803876876831\n",
      "Epoch: 6\tBatch: 500\tAvg-Loss: 5.1498\n",
      "Time: 37.80111103455226 \n",
      "\n",
      "Epoch: 6\tBatch: 550\tAvg-Loss: 5.1354\n",
      "Time: 38.02991915543874 \n",
      "\n",
      "Scheduler saw loss: 5.133578481674195\n",
      "Epoch: 6\tBatch: 600\tAvg-Loss: 5.1336\n",
      "Time: 38.258142268657686 \n",
      "\n",
      "Epoch: 6\tBatch: 650\tAvg-Loss: 5.1149\n",
      "Time: 38.48651884396871 \n",
      "\n",
      "Scheduler saw loss: 5.1232168579101565\n",
      "Epoch: 6\tBatch: 700\tAvg-Loss: 5.1232\n",
      "Time: 38.714872336387636 \n",
      "\n",
      "Epoch: 6\tBatch: 750\tAvg-Loss: 5.1139\n",
      "Time: 38.94324450890223 \n",
      "\n",
      "Scheduler saw loss: 5.092848567962647\n",
      "Epoch: 6\tBatch: 800\tAvg-Loss: 5.0928\n",
      "Time: 39.17160260279973 \n",
      "\n",
      "Epoch: 6\tBatch: 850\tAvg-Loss: 5.0622\n",
      "Time: 39.39989095131556 \n",
      "\n",
      "Scheduler saw loss: 5.0835270500183105\n",
      "Epoch: 6\tBatch: 900\tAvg-Loss: 5.0835\n",
      "Time: 39.62822300195694 \n",
      "\n",
      "Epoch: 6\tBatch: 950\tAvg-Loss: 5.0589\n",
      "Time: 39.85652285814285 \n",
      "\n",
      "Scheduler saw loss: 5.069853458404541\n",
      "Epoch: 6\tBatch: 1000\tAvg-Loss: 5.0699\n",
      "Time: 40.084777267773944 \n",
      "\n",
      "Epoch: 6\tBatch: 1050\tAvg-Loss: 5.0466\n",
      "Time: 40.31312422355016 \n",
      "\n",
      "Scheduler saw loss: 5.039876136779785\n",
      "Epoch: 6\tBatch: 1100\tAvg-Loss: 5.0399\n",
      "Time: 40.54140320221583 \n",
      "\n",
      "Epoch: 6\tBatch: 1150\tAvg-Loss: 5.0211\n",
      "Time: 40.76972266832988 \n",
      "\n",
      "Scheduler saw loss: 5.022501420974732\n",
      "Epoch: 6\tBatch: 1200\tAvg-Loss: 5.0225\n",
      "Time: 40.998075532913205 \n",
      "\n",
      "Epoch: 6\tBatch: 1250\tAvg-Loss: 5.0166\n",
      "Time: 41.226692668596904 \n",
      "\n",
      "Scheduler saw loss: 5.053760986328125\n",
      "Epoch: 6\tBatch: 1300\tAvg-Loss: 5.0538\n",
      "Time: 41.45530181328456 \n",
      "\n",
      "Epoch: 6\tBatch: 1350\tAvg-Loss: 5.0038\n",
      "Time: 41.68392811218897 \n",
      "\n",
      "Scheduler saw loss: 4.988839483261108\n",
      "Epoch: 6\tBatch: 1400\tAvg-Loss: 4.9888\n",
      "Time: 41.912687178452806 \n",
      "\n",
      "Epoch: 6\tBatch: 1450\tAvg-Loss: 5.0089\n",
      "Time: 42.14163439273834 \n",
      "\n",
      "Epoch: 5 - Time: 42.30861052274704 \n",
      "\n",
      "ROC with Pairwise Distance 0.12063062760815202\n",
      "ROC Value: 0.8560243827534859\n",
      "Validation loss: 5.154808288574219, validation accuracy: 0.089\n",
      "Epoch: 7\tBatch: 50\tAvg-Loss: 4.8655\n",
      "Time: 42.84956378539403 \n",
      "\n",
      "Scheduler saw loss: 4.867575187683105\n",
      "Epoch: 7\tBatch: 100\tAvg-Loss: 4.8676\n",
      "Time: 43.077451694011685 \n",
      "\n",
      "Epoch: 7\tBatch: 150\tAvg-Loss: 4.8666\n",
      "Time: 43.30565656820933 \n",
      "\n",
      "Scheduler saw loss: 4.881539459228516\n",
      "Epoch: 7\tBatch: 200\tAvg-Loss: 4.8815\n",
      "Time: 43.53365678787232 \n",
      "\n",
      "Epoch: 7\tBatch: 250\tAvg-Loss: 4.8597\n",
      "Time: 43.761595066388445 \n",
      "\n",
      "Scheduler saw loss: 4.8349196434021\n",
      "Epoch: 7\tBatch: 300\tAvg-Loss: 4.8349\n",
      "Time: 43.989476223786674 \n",
      "\n",
      "Epoch: 7\tBatch: 350\tAvg-Loss: 4.8710\n",
      "Time: 44.217353947957356 \n",
      "\n",
      "Scheduler saw loss: 4.8669898986816404\n",
      "Epoch: 7\tBatch: 400\tAvg-Loss: 4.8670\n",
      "Time: 44.445226124922435 \n",
      "\n",
      "Epoch: 7\tBatch: 450\tAvg-Loss: 4.8549\n",
      "Time: 44.67311389048894 \n",
      "\n",
      "Scheduler saw loss: 4.894142789840698\n",
      "Epoch: 7\tBatch: 500\tAvg-Loss: 4.8941\n",
      "Time: 44.90109380086263 \n",
      "\n",
      "Epoch: 7\tBatch: 550\tAvg-Loss: 4.8634\n",
      "Time: 45.129152158896126 \n",
      "\n",
      "Scheduler saw loss: 4.877794313430786\n",
      "Epoch: 7\tBatch: 600\tAvg-Loss: 4.8778\n",
      "Time: 45.35739229917526 \n",
      "\n",
      "Epoch: 7\tBatch: 650\tAvg-Loss: 4.8161\n",
      "Time: 45.58521554867426 \n",
      "\n",
      "Scheduler saw loss: 4.811386833190918\n",
      "Epoch: 7\tBatch: 700\tAvg-Loss: 4.8114\n",
      "Time: 45.813124235471086 \n",
      "\n",
      "Epoch: 7\tBatch: 750\tAvg-Loss: 4.8338\n",
      "Time: 46.040989792346956 \n",
      "\n",
      "Scheduler saw loss: 4.822868185043335\n",
      "Epoch: 7\tBatch: 800\tAvg-Loss: 4.8229\n",
      "Time: 46.269282718499504 \n",
      "\n",
      "Epoch: 7\tBatch: 850\tAvg-Loss: 4.8339\n",
      "Time: 46.49720919132233 \n",
      "\n",
      "Scheduler saw loss: 4.86312686920166\n",
      "Epoch: 7\tBatch: 900\tAvg-Loss: 4.8631\n",
      "Time: 46.72513891061147 \n",
      "\n",
      "Epoch: 7\tBatch: 950\tAvg-Loss: 4.8649\n",
      "Time: 46.95329247315725 \n",
      "\n",
      "Scheduler saw loss: 4.8388742828369145\n",
      "Epoch: 7\tBatch: 1000\tAvg-Loss: 4.8389\n",
      "Time: 47.18181281487147 \n",
      "\n",
      "Epoch: 7\tBatch: 1050\tAvg-Loss: 4.8295\n",
      "Time: 47.40982977151871 \n",
      "\n",
      "Scheduler saw loss: 4.86321533203125\n",
      "Epoch: 7\tBatch: 1100\tAvg-Loss: 4.8632\n",
      "Time: 47.638051974773404 \n",
      "\n",
      "Epoch: 7\tBatch: 1150\tAvg-Loss: 4.8037\n",
      "Time: 47.86610079606374 \n",
      "\n",
      "Scheduler saw loss: 4.8373097324371335\n",
      "Epoch: 7\tBatch: 1200\tAvg-Loss: 4.8373\n",
      "Time: 48.09418263038 \n",
      "\n",
      "Epoch: 7\tBatch: 1250\tAvg-Loss: 4.8051\n",
      "Time: 48.322278861204786 \n",
      "\n",
      "Scheduler saw loss: 4.828385066986084\n",
      "Epoch: 7\tBatch: 1300\tAvg-Loss: 4.8284\n",
      "Time: 48.550206136703494 \n",
      "\n",
      "Epoch: 7\tBatch: 1350\tAvg-Loss: 4.8090\n",
      "Time: 48.77874942223231 \n",
      "\n",
      "Scheduler saw loss: 4.7988394832611085\n",
      "Epoch: 7\tBatch: 1400\tAvg-Loss: 4.7988\n",
      "Time: 49.006808722019194 \n",
      "\n",
      "Epoch: 7\tBatch: 1450\tAvg-Loss: 4.7673\n",
      "Time: 49.234836045900984 \n",
      "\n",
      "Epoch: 6 - Time: 49.401369293530784 \n",
      "\n",
      "ROC with Pairwise Distance 0.13155664577134202\n",
      "ROC Value: 0.8603658961461933\n",
      "Validation loss: 5.068293792724609, validation accuracy: 0.0985\n",
      "Epoch: 8\tBatch: 50\tAvg-Loss: 4.6445\n",
      "Time: 49.942378743489584 \n",
      "\n",
      "Scheduler saw loss: 4.664071063995362\n",
      "Epoch: 8\tBatch: 100\tAvg-Loss: 4.6641\n",
      "Time: 50.17076868613561 \n",
      "\n",
      "Epoch: 8\tBatch: 150\tAvg-Loss: 4.6841\n",
      "Time: 50.39931422472 \n",
      "\n",
      "Scheduler saw loss: 4.692284145355225\n",
      "Epoch: 8\tBatch: 200\tAvg-Loss: 4.6923\n",
      "Time: 50.62766575018565 \n",
      "\n",
      "Epoch: 8\tBatch: 250\tAvg-Loss: 4.7012\n",
      "Time: 50.85581878423691 \n",
      "\n",
      "Scheduler saw loss: 4.661803274154663\n",
      "Epoch: 8\tBatch: 300\tAvg-Loss: 4.6618\n",
      "Time: 51.08418772220612 \n",
      "\n",
      "Epoch: 8\tBatch: 350\tAvg-Loss: 4.6639\n",
      "Time: 51.31242575248083 \n",
      "\n",
      "Scheduler saw loss: 4.662877082824707\n",
      "Epoch: 8\tBatch: 400\tAvg-Loss: 4.6629\n",
      "Time: 51.541043535868326 \n",
      "\n",
      "Epoch: 8\tBatch: 450\tAvg-Loss: 4.7024\n",
      "Time: 51.76930043299993 \n",
      "\n",
      "Scheduler saw loss: 4.673432788848877\n",
      "Epoch: 8\tBatch: 500\tAvg-Loss: 4.6734\n",
      "Time: 51.99762922128041 \n",
      "\n",
      "Epoch: 8\tBatch: 550\tAvg-Loss: 4.6904\n",
      "Time: 52.225947145620985 \n",
      "\n",
      "Scheduler saw loss: 4.665985889434815\n",
      "Epoch: 8\tBatch: 600\tAvg-Loss: 4.6660\n",
      "Time: 52.454251984755196 \n",
      "\n",
      "Epoch: 8\tBatch: 650\tAvg-Loss: 4.6842\n",
      "Time: 52.682605385780334 \n",
      "\n",
      "Scheduler saw loss: 4.687927932739258\n",
      "Epoch: 8\tBatch: 700\tAvg-Loss: 4.6879\n",
      "Time: 52.911895501613614 \n",
      "\n",
      "Epoch: 8\tBatch: 750\tAvg-Loss: 4.6810\n",
      "Time: 53.14045898119608 \n",
      "\n",
      "Scheduler saw loss: 4.634023923873901\n",
      "Epoch: 8\tBatch: 800\tAvg-Loss: 4.6340\n",
      "Time: 53.36909672021866 \n",
      "\n",
      "Epoch: 8\tBatch: 850\tAvg-Loss: 4.6394\n",
      "Time: 53.59730699062347 \n",
      "\n",
      "Scheduler saw loss: 4.6450550365448\n",
      "Epoch: 8\tBatch: 900\tAvg-Loss: 4.6451\n",
      "Time: 53.82627481619517 \n",
      "\n",
      "Epoch: 8\tBatch: 950\tAvg-Loss: 4.7075\n",
      "Time: 54.05459027290344 \n",
      "\n",
      "Scheduler saw loss: 4.670622396469116\n",
      "Epoch: 8\tBatch: 1000\tAvg-Loss: 4.6706\n",
      "Time: 54.282940510908766 \n",
      "\n",
      "Epoch: 8\tBatch: 1050\tAvg-Loss: 4.6894\n",
      "Time: 54.51210689544678 \n",
      "\n",
      "Scheduler saw loss: 4.6745811939239506\n",
      "Epoch: 8\tBatch: 1100\tAvg-Loss: 4.6746\n",
      "Time: 54.74051652352015 \n",
      "\n",
      "Epoch: 8\tBatch: 1150\tAvg-Loss: 4.6849\n",
      "Time: 54.96901462872823 \n",
      "\n",
      "Scheduler saw loss: 4.642317571640015\n",
      "Epoch: 8\tBatch: 1200\tAvg-Loss: 4.6423\n",
      "Time: 55.19743121465047 \n",
      "\n",
      "Epoch: 8\tBatch: 1250\tAvg-Loss: 4.6885\n",
      "Time: 55.42573881149292 \n",
      "\n",
      "Scheduler saw loss: 4.635033617019653\n",
      "Epoch: 8\tBatch: 1300\tAvg-Loss: 4.6350\n",
      "Time: 55.65404358704885 \n",
      "\n",
      "Epoch: 8\tBatch: 1350\tAvg-Loss: 4.6281\n",
      "Time: 55.8824348171552 \n",
      "\n",
      "Scheduler saw loss: 4.628828086853027\n",
      "Epoch: 8\tBatch: 1400\tAvg-Loss: 4.6288\n",
      "Time: 56.111074634393056 \n",
      "\n",
      "Epoch: 8\tBatch: 1450\tAvg-Loss: 4.6448\n",
      "Time: 56.339596911271414 \n",
      "\n",
      "Epoch: 7 - Time: 56.50644317865372 \n",
      "\n",
      "ROC with Pairwise Distance 0.1176955613994792\n",
      "ROC Value: 0.8510115178057587\n",
      "Validation loss: 4.818161956787109, validation accuracy: 0.120625\n",
      "Epoch: 9\tBatch: 50\tAvg-Loss: 4.4828\n",
      "Time: 57.048552958170575 \n",
      "\n",
      "Scheduler saw loss: 4.509378442764282\n",
      "Epoch: 9\tBatch: 100\tAvg-Loss: 4.5094\n",
      "Time: 57.27671204805374 \n",
      "\n",
      "Epoch: 9\tBatch: 150\tAvg-Loss: 4.4706\n",
      "Time: 57.505032149950665 \n",
      "\n",
      "Scheduler saw loss: 4.5500861930847165\n",
      "Epoch: 9\tBatch: 200\tAvg-Loss: 4.5501\n",
      "Time: 57.73322020769119 \n",
      "\n",
      "Epoch: 9\tBatch: 250\tAvg-Loss: 4.5210\n",
      "Time: 57.96155539751053 \n",
      "\n",
      "Scheduler saw loss: 4.5331923770904545\n",
      "Epoch: 9\tBatch: 300\tAvg-Loss: 4.5332\n",
      "Time: 58.19000385204951 \n",
      "\n",
      "Epoch: 9\tBatch: 350\tAvg-Loss: 4.5147\n",
      "Time: 58.418569684028625 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ca479568c9bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_val_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-511f1ef27fcb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, test_loader, task)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train(basic_net,train_dataloader,test_val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQvxVKSfP3P8"
   },
   "source": [
    "## Submitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TwL8cEW0HfUB",
    "outputId": "89318785-a0c2-4bba-e4aa-c478c106626c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4)\n",
      "100% 3.61M/3.61M [00:00<00:00, 14.3MB/s]\n",
      "Successfully submitted to 11785-HW2p2-slack-kaggle"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "df=Submission(basic_net, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "34zHn9ixPY27"
   },
   "outputs": [],
   "source": [
    "Version = '1'\n",
    "File_name = 'My_Model_HW2_V' + Version + Model_type + '.pt'\n",
    "torch.save(network.state_dict(), File_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "OZXkCkQAN4GN",
    "kyznzSkUQHz1",
    "LxrMiMpxHhFY"
   ],
   "machine_shape": "hm",
   "name": "HW2_P2_Jupyter_Custom-ARC-3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
